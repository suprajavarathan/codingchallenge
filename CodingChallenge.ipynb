{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91c6a418-64dd-46d4-82b9-15204807fc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered Account Ids:\n",
      "+--------------------------------+-----------+\n",
      "|HashKey                         |AccountId  |\n",
      "+--------------------------------+-----------+\n",
      "|0f6f2b86bf61c32a4e6a24a21c677655|15368      |\n",
      "|a233b2e3f632c09b0df252165c112f61|23232      |\n",
      "|a585cbf86a677cb100929ac2cc2c6ba9|232323     |\n",
      "|c448e1abaeefbe05a8847d1fbdc7fe4e|307263     |\n",
      "|0623963d62f3f2d15bc63621e5297d2f|415740     |\n",
      "|66ca8eb73ba003fc9fb0f61023fcfc39|524218     |\n",
      "|062b118ef57802ee9674862012d336b8|632695     |\n",
      "|2fa9ae5b103842545d942aabbdb7aa8d|741173     |\n",
      "|ed37cdb0bb927f08e08afb9e20f9263c|849650     |\n",
      "|71e745d0418e495a8e437e2659989bef|958128     |\n",
      "|df634d91c34119fb67a4fc254a229367|1066605    |\n",
      "|da05891a363fc2be957c86df9570b199|1175082.667|\n",
      "|ad80095218c7357f95602cb06bc0547d|1283560.167|\n",
      "|55eee16de2f48d3d24d74a71e0df6807|1392037.667|\n",
      "|c782cbdff011c69de5ef501c22c30f56|1500515.167|\n",
      "|e59dcebc464fd04d0e527acec89436c8|1608992.667|\n",
      "|b3a987929956ce94e408fe65c7c976b6|1717470.167|\n",
      "|1fa832e58e2d3307fca12a894152700e|1825947.667|\n",
      "|cbb0040402c13e83bf0ab7230c98c579|1934425.167|\n",
      "|2192a33d2ed81f3be0b85b28d3a29336|2042902.667|\n",
      "+--------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Count before removing the Account IDs: 30320\n",
      "***********************************************\n",
      "Output validation\n",
      "Count after removing the non-integer Account IDs: 30308\n",
      "***********************************************\n",
      "Unfiltered Fibre records:\n",
      "+--------------------------------+---------+----------------------+\n",
      "|HashKey                         |AccountID|Fibre                 |\n",
      "+--------------------------------+---------+----------------------+\n",
      "|0f6f2b86bf61c32a4e6a24a21c677655|15368    |EN121794032-N-5       |\n",
      "|a233b2e3f632c09b0df252165c112f61|23232    |EN121542407-N-12      |\n",
      "|a585cbf86a677cb100929ac2cc2c6ba9|232323   |EN121491293-N-9       |\n",
      "|c448e1abaeefbe05a8847d1fbdc7fe4e|307263   |EN121491293-N-13      |\n",
      "|0623963d62f3f2d15bc63621e5297d2f|415740   |EA983_54004586547-N-11|\n",
      "|66ca8eb73ba003fc9fb0f61023fcfc39|524218   |EA983_146001477792-N-3|\n",
      "|062b118ef57802ee9674862012d336b8|632695   |EA983_50004634622-N-13|\n",
      "|2fa9ae5b103842545d942aabbdb7aa8d|741173   |2.67E+14              |\n",
      "|ed37cdb0bb927f08e08afb9e20f9263c|849650   |2.67E+14              |\n",
      "|71e745d0418e495a8e437e2659989bef|958128   |2.67E+14              |\n",
      "|df634d91c34119fb67a4fc254a229367|1066605  |2.67E+14              |\n",
      "|016eb19c3f19119aabbb7f5015fbb88a|200098   |EN121653266-N-9       |\n",
      "|33f2389000f2597d96763bc68128250f|10033    |EN121680822-N-8       |\n",
      "|08d67c247e8afaf4261ec8dbe8d89615|10049    |EN121524147-N-16      |\n",
      "|4ba2ac25a02265acf127559a8ea142b0|10066    |EN121508087-N-3       |\n",
      "|0820b0031f40dfafbe128af95179314f|10033    |EN121519443-N-8       |\n",
      "|2231fbabb03774a19f8b791264f92aee|10055    |EN121495792-N-18      |\n",
      "|181ce106be0897b8672fcb7ca92e8884|10063    |EN121667218-N-7       |\n",
      "|6b31b4273674f15b54bc92bef1322558|10001    |EN121659042-N-9       |\n",
      "|3d1a1754add70f80bdf90e4eb8700e84|10059    |EN121716850-N-13      |\n",
      "+--------------------------------+---------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "***********************************************\n",
      "Count after before removing special characters from Fibre column: 29501\n",
      "***********************************************\n",
      "Fastest Response based on PostCode\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+--------+----------------+\n",
      "|PostCode|MinDaysToRespond|\n",
      "+--------+----------------+\n",
      "|    2587|               1|\n",
      "|    2582|               1|\n",
      "|    2581|               1|\n",
      "|    2586|               1|\n",
      "|    2722|               1|\n",
      "|    2081|               1|\n",
      "+--------+----------------+\n",
      "\n",
      "Top Agent based on postcode and Amount\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "+-------+--------+---------------+\n",
      "|AgentID|PostCode|AmountCollected|\n",
      "+-------+--------+---------------+\n",
      "| 306773|    2584|         205.58|\n",
      "| 307312|    2581|        6915.03|\n",
      "| 307508|    2587|       58978.79|\n",
      "| 307509|    2586|        3542.46|\n",
      "| 307561|    2626|          22.18|\n",
      "| 307562|    2582|         -12.05|\n",
      "| 307564|    2081|       54922.77|\n",
      "| 307662|    2722|        1274.57|\n",
      "+-------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "from json import JSONEncoder\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Convert to Json file\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Cleanse the csv file\n",
    "class Cleanse_file:\n",
    "    def __init__(self,df2):\n",
    "        df_clean = df_csv.withColumnRenamed(\"Account_ID \", \"AccountID\")\\\n",
    "                               .withColumnRenamed(\"CODE \",\"Code\")\\\n",
    "                               .withColumnRenamed(\"Active Indicator \",\"ActiveIndicator\")\\\n",
    "                               .withColumnRenamed(\"Implemented Date \",\"ImplementedDate\")\\\n",
    "                               .withColumnRenamed(\"Account Type \",\"AccountType\")\\\n",
    "                               .withColumnRenamed(\"Service \",\"Service\")\\\n",
    "                               .withColumnRenamed(\"BU\",\"BU\")\\\n",
    "                               .withColumnRenamed(\"Request Date \",\"RequestDate\")\\\n",
    "                               .withColumnRenamed(\"Account status \",\"AccountStatus\")\\\n",
    "                               .withColumnRenamed(\"Status Code \",\"StatusCode\")\\\n",
    "                               .withColumnRenamed(\"$ Amount \",\"Amount\")\\\n",
    "                               .withColumnRenamed(\"Version \",\"Version\")\\\n",
    "                               .withColumnRenamed(\"Agent ID \",\"AgentID\")\\\n",
    "                               .withColumnRenamed(\"FIBRE \", \"Fibre\")\\\n",
    "                               .withColumnRenamed(\"last Updated Date \",\"LastUpdatedDate\")\\\n",
    "                               .withColumnRenamed(\"Property TYPE \",\"PropertyType\")\\\n",
    "                               .withColumnRenamed(\"Post Code \",\"PostCode\")\n",
    "                        \n",
    "        # remove the timestamp column from the date field\n",
    "        df_clean = df_clean.withColumn(\"ImplementedDate\",expr(\"substring(ImplementedDate,1,length(ImplementedDate)-5)\")) \\\n",
    "                       .withColumn(\"RequestDate\",expr(\"substring(RequestDate,1,length(RequestDate)-5)\"))\n",
    "   \n",
    "        # Convert the string formatted date column to Date format and Amount to datatype float\n",
    "        # Generate hashkey\n",
    "        df_clean = df_clean.withColumn(\"ImplementedDate\",when(length(col(\"ImplementedDate\")) == 9,concat(lit(0),col(\"ImplementedDate\")) ).otherwise(col(\"ImplementedDate\")))\\\n",
    "                       .withColumn(\"RequestDate\",when(length(col(\"RequestDate\")) == 9,concat(lit(0),col(\"RequestDate\")) ).otherwise(col(\"RequestDate\"))) \\\n",
    "                       .withColumn(\"Amount\",df_clean.Amount.cast('float'))\\\n",
    "                       .withColumn(\"HashKey\",md5(concat(col(\"AccountID\"),col(\"Fibre\"))))  # Generate Hashkey based on accountid and Fibre column\n",
    "    \n",
    " \n",
    "        df_clean1 = df_clean.select(\"HashKey\",\"AccountID\",\"Code\",\"ActiveIndicator\",\\\n",
    "                                    to_date(col(\"ImplementedDate\"),\"dd/MM/yyyy\").alias(\"ImplementedDate\"),\\\n",
    "                                    \"AccountType\",\"Service\",\"BU\",\\\n",
    "                                    to_date(col(\"RequestDate\"),\"dd/MM/yyyy\").alias(\"RequestDate\"),\\\n",
    "                                    \"AccountStatus\",\"StatusCode\",\"Amount\",\"Version\",\"AgentID\",\"Fibre\",\"LastUpdatedDate\",\"PropertyType\",\\\n",
    "                                    \"PostCode\")\n",
    "  \n",
    "\n",
    "   \n",
    "        print(\"Unfiltered Account Ids:\")\n",
    "        df_clean1.select(\"HashKey\",\"AccountId\").show(truncate=False)\n",
    "        print(\"Count before removing the Account IDs:\",df_clean1.count())\n",
    "        \n",
    "         #filter decimal numbers from account column \n",
    "        #Account number will integer values is taken\n",
    "        df1 = df_clean1.where(col(\"AccountID\").rlike(\"^[0-9]*$\") )\n",
    "        print (\"***********************************************\")\n",
    "        print (\"Output validation\")\n",
    "        print (\"Count after removing the non-integer Account IDs:\", df1.count())\n",
    "        print (\"***********************************************\")\n",
    "       \n",
    "   \n",
    "    # filter data with + sign from Fibre column\n",
    "    \n",
    "        df1.createOrReplaceTempView(\"Data\")\n",
    "        print(\"Unfiltered Fibre records:\")\n",
    "        spark.sql(\"Select HashKey,AccountID,Fibre FROM Data\").show(truncate= False)\n",
    "        cleanfile = spark.sql(\"Select * from Data where Fibre  not like '%+%'\")\n",
    "        print (\"***********************************************\")\n",
    "        print (\"Count after before removing special characters from Fibre column:\", cleanfile.count())\n",
    "        print (\"***********************************************\")\n",
    "        \n",
    "        #Create Json file\n",
    "        jsonfile = Create_json_file(cleanfile)\n",
    "        \n",
    "        #Class which defines rest of  operations with data\n",
    "        fastresponse = Fast_response(cleanfile)\n",
    "        \n",
    "\n",
    "    # class to perform Top agents and fast response    \n",
    "class Fast_response:\n",
    "    def __init__(self,cleanfile):\n",
    "         # Fastest response based on postcode\n",
    "        print(\"Fastest Response based on PostCode\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        df3 = cleanfile.select(\"*\",datediff(col(\"ImplementedDate\"),col(\"RequestDate\")).alias(\"DaysToRespond\"))\n",
    "        df3.groupby(\"PostCode\").agg(min(\"DaysToRespond\").alias(\"MinDaysToRespond\")) \\\n",
    "           .where(col(\"MinDaysToRespond\") <= 1).show()\n",
    "    \n",
    "           # Top agent based on postcode and Amount\n",
    "        print(\"Top Agent based on postcode and Amount\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        df4 = cleanfile.groupby(\"AgentID\",\"PostCode\").agg(sum(\"Amount\").alias(\"TotalAmount\"))\n",
    "        df4.createOrReplaceTempView(\"TopAgents\")\n",
    "        TopAgentsByPostCode = spark.sql(\"SELECT a.AgentID,a.PostCode,cast(b.maxAmount as float) as AmountCollected from TopAgents a INNER JOIN \\\n",
    "                        (SELECT PostCode,Max(TotalAmount) as maxAmount from TopAgents\\\n",
    "                            group by PostCode) b \\\n",
    "                            on a.PostCode == b.PostCode and b.maxAmount=a.TotalAmount order by a.AgentID\" ).show()\n",
    "\n",
    "        # Class to create Json file\n",
    "class Create_json_file:\n",
    "    def __init__(self,cleanfile):        \n",
    "        json_list =[]\n",
    "        data_collect = cleanfile.collect()\n",
    "        \n",
    "        i=0 #initialise the counters for record writing\n",
    "        j=1 #intiialise for the file numbering\n",
    "        transaction = {'metadata':{'start_at':datetime.datetime.now(),'end_at':datetime.datetime.now()+datetime.timedelta(1)} }\n",
    "        transaction['data'] =[]\n",
    "        for row in data_collect:\n",
    "            if i <= 1000:\n",
    "               transaction['data'].append ({\n",
    "                        'HashKey' : row[\"HashKey\"],\n",
    "                        'AccountId' : row[\"AccountID\"],\n",
    "                        'Code' : row[\"Code\"],\n",
    "                        'ActiveIndicator' : row[\"ActiveIndicator\"],\n",
    "                        'ImplementedDate' : row[\"ImplementedDate\"],\n",
    "                        'AccountType' : row[\"AccountType\"],\n",
    "                        'Service' : row[\"Service\"],\n",
    "                        'BU': row[\"BU\"],\n",
    "                        'RequestDate' : row[\"RequestDate\"],\n",
    "                        'AccountStatus' : row[\"AccountStatus\"],\n",
    "                        'StatusCode' : row[\"StatusCode\"],\n",
    "                        'Amount' : row[\"Amount\"],\n",
    "                        'Version' : row[\"Version\"],\n",
    "                        'AgentID' : row[\"AgentID\"],\n",
    "                        'Fibre' : row[\"Fibre\"],\n",
    "                        'LastUpdatedDate' : row[\"LastUpdatedDate\"],\n",
    "                        'PropertyType' : row[\"PropertyType\"],\n",
    "                        'PostCode' : row[\"PostCode\"],\n",
    "               })\n",
    "               \n",
    "               i= i + 1\n",
    "            else:\n",
    "               i = 0\n",
    "                # Write into the file\n",
    "               with open(str(j) + \".json\", \"w\") as write_file:\n",
    "                  json.dump(transaction, write_file, indent=2,cls=DateTimeEncoder)\n",
    "             #  print(json_list)\n",
    "                # empty the list\n",
    "               transaction['data'] =[]\n",
    "               j = j + 1 \n",
    "          #write the remaining records\n",
    "        with open(str(j) + \".json\", \"w\") as write_file:\n",
    "                  json.dump(transaction, write_file, indent=2,cls=DateTimeEncoder)\n",
    "    \n",
    "# Class JSONEncoder for converting date format to returns isoformat\n",
    "class DateTimeEncoder(JSONEncoder):\n",
    "        # Override the default method\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, (datetime.date, datetime.datetime)):\n",
    "                return obj.isoformat()\n",
    "        \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # read a csv file\n",
    "    df_csv = spark.read.csv('Transaction.csv',header = True)\n",
    "        \n",
    "        #return cleanfile\n",
    "    activity = Cleanse_file(df_csv)\n",
    "    \n",
    "    \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157b998-eb26-49d3-8fdc-882e88d4bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
